import streamlit as st
import requests
import json
import os
import re
import time
import uuid
import sqlite3
from datetime import datetime
import tiktoken
from typing import List, Dict, Any, Optional, Generator
import hashlib
from functools import wraps
import io
import traceback

# === Konfiguracja ===
MODEL_OPTIONS = [
    {
        "id": "anthropic/claude-3.7-sonnet:floor",
        "name": "Claude 3.7 Sonnet",
        "pricing": {"prompt": 3.0, "completion": 15.0},
        "description": "Zalecany - Najnowszy model Claude z doskona≈Çymi umiejƒôtno≈õciami kodowania"
    },
    {
        "id": "anthropic/claude-3.7-sonnet:thinking",
        "name": "Claude 3.7 Sonnet Thinking",
        "pricing": {"prompt": 3.0, "completion": 15.0},
        "description": "Model Claude wykorzystujƒÖcy dodatkowy czas na analizƒô problem√≥w"
    },
    {
        "id": "openai/gpt-4o:floor",
        "name": "GPT-4o",
        "pricing": {"prompt": 2.5, "completion": 10.0},
        "description": "Silna alternatywa z dobrymi zdolno≈õciami kodowania"
    },
    {
        "id": "openai/gpt-4-turbo:floor",
        "name": "GPT-4 Turbo",
        "pricing": {"prompt": 2.5, "completion": 10.0},
        "description": "Nieco starszy model GPT-4 Turbo"
    },
    {
        "id": "anthropic/claude-3.5-haiku:floor",
        "name": "Claude 3.5 Haiku",
        "pricing": {"prompt": 0.8, "completion": 4.0},
        "description": "Szybszy, ta≈Ñszy model do prostszych zada≈Ñ"
    }
]

DEFAULT_SYSTEM_PROMPT = """Jeste≈õ ekspertkim asystentem specjalizujƒÖcym siƒô w tworzeniu aplikacji Streamlit wykorzystujƒÖcych AI. 
Pomagasz projektowaƒá, kodowaƒá i optymalizowaƒá aplikacje Streamlit, szczeg√≥lnie te korzystajƒÖce z modeli jƒôzykowych i innych us≈Çug AI.

Twoja wiedza specjalistyczna obejmuje:
1. Pisanie czystego, efektywnego kodu Streamlit
2. Projektowanie skutecznych interfejs√≥w u≈ºytkownika wykorzystujƒÖcych AI
3. Integracjƒô z API jak OpenRouter, OpenAI, Anthropic, itp.
4. Optymalizacjƒô wydajno≈õci i koszt√≥w przy korzystaniu z us≈Çug AI
5. Wdra≈ºanie najlepszych praktyk dla aplikacji Streamlit

Gdy podajesz przyk≈Çady kodu, przestrzegaj tych zasad:
- Do≈ÇƒÖczaj kompletne, dzia≈ÇajƒÖce rozwiƒÖzania, kt√≥re mo≈ºna skopiowaƒá i u≈ºyƒá bezpo≈õrednio
- Dodawaj kr√≥tkie komentarze wyja≈õniajƒÖce z≈Ço≈ºone czƒô≈õci
- Formatuj kod z odpowiednim wciƒôciem i strukturƒÖ
- Skup siƒô na najlepszych praktykach i efektywnych wzorcach Streamlit

Zawsze dziel aplikacje na logiczne komponenty i funkcje, zamiast pisaƒá wszystko w jednym bloku kodu.
Pamiƒôtaj o zarzƒÖdzaniu stanem sesji w Streamlit i optymalizacji koszt√≥w przy korzystaniu z API modeli jƒôzykowych.
"""

# === Funkcja dekoratora dla autoryzacji ===
def requires_auth(f):
    @wraps(f)
    def decorated(*args, **kwargs):
        if not st.session_state.get("authenticated", False):
            return login_page()
        return f(*args, **kwargs)
    return decorated

def login_page():
    """Strona logowania"""
    st.title("üîí Logowanie")
    
    col1, col2, col3 = st.columns([1, 2, 1])
    
    with col2:
        st.markdown("### Zaloguj siƒô, aby uzyskaƒá dostƒôp")
        
        username = st.text_input("Nazwa u≈ºytkownika", key="login_username")
        password = st.text_input("Has≈Ço", type="password", key="login_password")
        
        if st.button("Zaloguj"):
            # Pobierz ustawienia z secrets
            correct_username = st.secrets.get("APP_USER", "admin")
            correct_password = st.secrets.get("APP_PASSWORD", "password")
            
            if username == correct_username and password == correct_password:
                st.session_state["authenticated"] = True
                st.success("Zalogowano pomy≈õlnie!")
                st.rerun()
            else:
                st.error("Nieprawid≈Çowa nazwa u≈ºytkownika lub has≈Ço!")

# === ZarzƒÖdzanie bazƒÖ danych ===
class AssistantDB:
    def __init__(self, db_path='streamlit_assistant.db'):
        """Inicjalizacja po≈ÇƒÖczenia z bazƒÖ danych i tabel"""
        self.conn = sqlite3.connect(db_path, check_same_thread=False)
        self._create_tables()
    
    def _create_tables(self):
        """Utw√≥rz tabele bazy danych, je≈õli nie istniejƒÖ"""
        cursor = self.conn.cursor()
        
        # Tabela konwersacji
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS conversations (
            id TEXT PRIMARY KEY,
            title TEXT,
            created_at TIMESTAMP,
            updated_at TIMESTAMP
        )
        ''')
        
        # Tabela wiadomo≈õci
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS messages (
            id TEXT PRIMARY KEY,
            conversation_id TEXT,
            role TEXT,
            content TEXT,
            timestamp TIMESTAMP,
            attachments TEXT,
            FOREIGN KEY (conversation_id) REFERENCES conversations (id)
        )
        ''')
        
        self.conn.commit()

    def save_message(self, conversation_id: str, role: str, content: str, attachments=None) -> str:
        """Zapisz wiadomo≈õƒá w bazie danych"""
        cursor = self.conn.cursor()
        message_id = str(uuid.uuid4())
        
        # Przygotuj za≈ÇƒÖczniki do zapisu (konwersja danych binarnych)
        serializable_attachments = []
        if attachments:
            for attachment in attachments:
                # Tworzymy nowy s≈Çownik zawierajƒÖcy tylko serializowalne dane
                serialized = {
                    "type": attachment.get("type", ""),
                    "name": attachment.get("name", "")
                }
                
                # Je≈õli jest text_content, dodajemy go
                if "text_content" in attachment:
                    serialized["text_content"] = attachment["text_content"]
                
                # Nie zapisujemy binarnych danych w bazie danych
                serializable_attachments.append(serialized)
        
        attachments_json = json.dumps(serializable_attachments)
        
        cursor.execute(
            "INSERT INTO messages (id, conversation_id, role, content, timestamp, attachments) VALUES (?, ?, ?, ?, ?, ?)",
            (message_id, conversation_id, role, content, datetime.now(), attachments_json)
        )
        self.conn.commit()
        return message_id
    
    def get_messages(self, conversation_id: str) -> List[Dict[str, Any]]:
        """Pobierz wszystkie wiadomo≈õci dla konwersacji"""
        cursor = self.conn.cursor()
        cursor.execute(
            "SELECT role, content, attachments FROM messages WHERE conversation_id = ? ORDER BY timestamp",
            (conversation_id,)
        )
        
        messages = []
        for role, content, attachments_json in cursor.fetchall():
            try:
                attachments = json.loads(attachments_json) if attachments_json else []
                
                messages.append({
                    "role": role,
                    "content": content,
                    "attachments": attachments
                })
            except Exception as e:
                # W przypadku b≈Çƒôdu, dodaj wiadomo≈õƒá bez za≈ÇƒÖcznik√≥w
                messages.append({
                    "role": role,
                    "content": content,
                    "attachments": []
                })
                
        return messages

    def get_messages_with_pagination(self, conversation_id: str, limit: int = None, offset: int = 0) -> List[Dict[str, Any]]:
        """Pobierz wiadomo≈õci dla konwersacji z paginacjƒÖ"""
        cursor = self.conn.cursor()
        
        if limit is not None:
            cursor.execute(
                "SELECT role, content, attachments FROM messages WHERE conversation_id = ? ORDER BY timestamp LIMIT ? OFFSET ?",
                (conversation_id, limit, offset)
            )
        else:
            cursor.execute(
                "SELECT role, content, attachments FROM messages WHERE conversation_id = ? ORDER BY timestamp",
                (conversation_id,)
            )
        
        messages = []
        for role, content, attachments_json in cursor.fetchall():
            try:
                attachments = json.loads(attachments_json) if attachments_json else []
                
                messages.append({
                    "role": role,
                    "content": content,
                    "attachments": attachments
                })
            except Exception as e:
                print(f"B≈ÇƒÖd przetwarzania za≈ÇƒÖcznika: {str(e)}")
                messages.append({
                    "role": role,
                    "content": content,
                    "attachments": []
                })
                
        return messages

    def get_message_count(self, conversation_id: str) -> int:
        """Pobierz liczbƒô wiadomo≈õci w konwersacji"""
        cursor = self.conn.cursor()
        cursor.execute(
            "SELECT COUNT(*) FROM messages WHERE conversation_id = ?",
            (conversation_id,)
        )
        return cursor.fetchone()[0]

    def save_conversation(self, conversation_id: str, title: str):
        """Utw√≥rz lub zaktualizuj konwersacjƒô"""
        cursor = self.conn.cursor()
        now = datetime.now()
        
        # Sprawd≈∫, czy konwersacja istnieje
        cursor.execute("SELECT id FROM conversations WHERE id = ?", (conversation_id,))
        if cursor.fetchone():
            # Aktualizuj istniejƒÖcƒÖ konwersacjƒô
            cursor.execute(
                "UPDATE conversations SET title = ?, updated_at = ? WHERE id = ?",
                (title, now, conversation_id)
            )
        else:
            # Utw√≥rz nowƒÖ konwersacjƒô
            cursor.execute(
                "INSERT INTO conversations (id, title, created_at, updated_at) VALUES (?, ?, ?, ?)",
                (conversation_id, title, now, now)
            )
        
        self.conn.commit()
    
    def get_conversations(self) -> List[Dict[str, Any]]:
        """Pobierz wszystkie konwersacje"""
        cursor = self.conn.cursor()
        cursor.execute(
            "SELECT id, title, created_at FROM conversations ORDER BY updated_at DESC"
        )
        return [
            {"id": conv_id, "title": title, "created_at": created_at} 
            for conv_id, title, created_at in cursor.fetchall()
        ]

    def delete_conversation(self, conversation_id: str):
        """Usu≈Ñ konwersacjƒô i jej wiadomo≈õci"""
        cursor = self.conn.cursor()
        # Najpierw usu≈Ñ wiadomo≈õci (ograniczenie klucza obcego)
        cursor.execute("DELETE FROM messages WHERE conversation_id = ?", (conversation_id,))
        # Usu≈Ñ konwersacjƒô
        cursor.execute("DELETE FROM conversations WHERE id = ?", (conversation_id,))
        self.conn.commit()

# === Funkcje pomocnicze dla zarzƒÖdzania kontekstem i za≈ÇƒÖcznikami ===
def prepare_messages_with_token_management(messages, system_prompt, model_id, llm_service):
    """Przygotowuje wiadomo≈õci do wys≈Çania, zarzƒÖdzajƒÖc limitami token√≥w"""
    
    # Ustal limit token√≥w dla r√≥≈ºnych modeli
    model_token_limits = {
        "anthropic/claude-3.7-sonnet:floor": 180000,
        "anthropic/claude-3.7-sonnet:thinking": 180000,
        "anthropic/claude-3.5-haiku:floor": 150000,
        "openai/gpt-4o:floor": 120000, 
        "openai/gpt-4-turbo:floor": 100000,
    }
    
    # Domy≈õlny limit, je≈õli model nie jest znany
    default_token_limit = 100000
    max_input_tokens = model_token_limits.get(model_id, default_token_limit)
    
    # Rezerwuj tokeny na odpowied≈∫ i prompt systemowy
    max_completion_tokens = 12000
    system_tokens = llm_service.count_tokens(system_prompt) if system_prompt else 0
    available_tokens = max_input_tokens - system_tokens - max_completion_tokens - 100  # 100 jako bufor bezpiecze≈Ñstwa
    
    # Przygotuj wiadomo≈õci API
    api_messages = []
    if system_prompt:
        api_messages.append({"role": "system", "content": system_prompt})
    
    # Policz tokeny aktualnych wiadomo≈õci
    current_tokens = 0
    user_messages = []
    assistant_messages = []
    
    # Najpierw zbierz wszystkie wiadomo≈õci
    for msg in messages:
        if msg["role"] == "user":
            user_messages.append(msg)
        else:
            assistant_messages.append(msg)
    
    # Zawsze do≈ÇƒÖcz ostatniƒÖ wiadomo≈õƒá u≈ºytkownika
    if user_messages:
        last_user_message = user_messages.pop()
    else:
        last_user_message = None
    
    # Zawsze do≈ÇƒÖcz ostatniƒÖ odpowied≈∫ asystenta, je≈õli istnieje
    if assistant_messages:
        last_assistant_message = assistant_messages.pop()
    else:
        last_assistant_message = None
    
    # Pierwsza wiadomo≈õƒá u≈ºytkownika jako kontekst, je≈õli istnieje
    if user_messages:
        first_user_message = user_messages.pop(0)
    else:
        first_user_message = None
    
    # Dodaj pierwszƒÖ wiadomo≈õƒá u≈ºytkownika do kontekstu
    if first_user_message:
        first_msg_tokens = llm_service.count_tokens(first_user_message["content"])
        if current_tokens + first_msg_tokens <= available_tokens:
            api_messages.append(first_user_message)
            current_tokens += first_msg_tokens
    
    # Proces dodawania pozosta≈Çych wiadomo≈õci w parach (zachowuje przep≈Çyw konwersacji)
    remaining_messages = []
    # ≈ÅƒÖczymy wiadomo≈õci z obu list naprzemiennie, zachowujƒÖc kolejno≈õƒá
    i, j = 0, 0
    while i < len(user_messages) or j < len(assistant_messages):
        if i < len(user_messages):
            remaining_messages.append(user_messages[i])
            i += 1
        if j < len(assistant_messages):
            remaining_messages.append(assistant_messages[j])
            j += 1
    
    # Sortuj pozosta≈Çe wiadomo≈õci wed≈Çug czasu (najstarsze pierwsze)
    # Zak≈Çadamy, ≈ºe wiadomo≈õci sƒÖ ju≈º uporzƒÖdkowane chronologicznie w bazie danych
    
    # Dodaj tyle wiadomo≈õci, ile zmie≈õci siƒô w limicie token√≥w
    for msg in remaining_messages:
        msg_tokens = llm_service.count_tokens(msg["content"])
        
        # Je≈õli wiadomo≈õƒá nie zmie≈õci siƒô, przerwij
        if current_tokens + msg_tokens > available_tokens:
            break
        
        api_messages.append(msg)
        current_tokens += msg_tokens
    
    # Zawsze dodaj ostatniƒÖ odpowied≈∫ asystenta, je≈õli istnieje i jest miejsce
    if last_assistant_message:
        last_assistant_tokens = llm_service.count_tokens(last_assistant_message["content"])
        if current_tokens + last_assistant_tokens <= available_tokens:
            api_messages.append(last_assistant_message)
            current_tokens += last_assistant_tokens
        else:
            # Je≈õli nie zmie≈õci siƒô ca≈Ça, dodaj skr√≥conƒÖ wersjƒô
            truncated_content = "POPRZEDNIA ODPOWIED≈π (skr√≥cona): " + last_assistant_message["content"][:1000] + "..."
            truncated_tokens = llm_service.count_tokens(truncated_content)
            if current_tokens + truncated_tokens <= available_tokens:
                api_messages.append({"role": "assistant", "content": truncated_content})
                current_tokens += truncated_tokens
    
    # Zawsze dodaj ostatniƒÖ wiadomo≈õƒá u≈ºytkownika
    if last_user_message:
        last_user_tokens = llm_service.count_tokens(last_user_message["content"])
        
        # Je≈õli ostatnia wiadomo≈õƒá u≈ºytkownika jest za d≈Çuga, spr√≥buj jƒÖ skr√≥ciƒá
        if current_tokens + last_user_tokens > available_tokens:
            # Oblicz, ile token√≥w mo≈ºemy u≈ºyƒá
            remaining_tokens = available_tokens - current_tokens
            
            # Je≈õli za ma≈Ço miejsca, dodaj tylko tre≈õƒá bez za≈ÇƒÖcznik√≥w
            if "attachments" in last_user_message and last_user_message["attachments"]:
                # Wyodrƒôbnij tre≈õƒá bez za≈ÇƒÖcznik√≥w
                main_content = last_user_message["content"].split("\n\n[Za≈ÇƒÖcznik")[0]
                main_content_tokens = llm_service.count_tokens(main_content)
                
                if main_content_tokens <= remaining_tokens:
                    # Dodaj tylko g≈Ç√≥wnƒÖ tre≈õƒá
                    modified_message = {"role": "user", "content": main_content}
                    api_messages.append(modified_message)
                else:
                    # Skr√≥ƒá nawet g≈Ç√≥wnƒÖ tre≈õƒá, je≈õli potrzeba
                    truncated_content = main_content[:remaining_tokens * 4]  # Przybli≈ºenie
                    modified_message = {"role": "user", "content": truncated_content}
                    api_messages.append(modified_message)
            else:
                # Skr√≥ƒá wiadomo≈õƒá u≈ºytkownika, je≈õli nie ma za≈ÇƒÖcznik√≥w
                truncated_content = last_user_message["content"][:remaining_tokens * 4]  # Przybli≈ºenie
                modified_message = {"role": "user", "content": truncated_content}
                api_messages.append(modified_message)
        else:
            # Dodaj pe≈ÇnƒÖ wiadomo≈õƒá
            api_messages.append(last_user_message)
    
    return api_messages

def process_attachments(attachments):
    """Przetwarza za≈ÇƒÖczniki w format odpowiedni do wys≈Çania do API"""
    processed_content = ""
    
    for attachment in attachments:
        att_type = attachment.get("type")
        att_name = attachment.get("name", "")
        
        if att_type == "file" and "text_content" in attachment:
            # Sprawd≈∫, czy zawarto≈õƒá to blok kodu
            if attachment['text_content'].startswith("```") and attachment['text_content'].endswith("```"):
                processed_content += f"\n\n[KOD: {att_name}]\n{attachment['text_content']}\n"
            else:
                processed_content += f"\n\n[PLIK: {att_name}]\n{attachment['text_content']}\n"
    
    return processed_content.strip()

def display_code_block(code, language="python"):
    """Wy≈õwietla blok kodu z opcjami kopiowania i pobrania"""
    st.code(code, language=language)
    
    col1, col2 = st.columns([1, 1])
    with col1:
        if st.button("Kopiuj kod", key=f"copy_{hash(code)}", use_container_width=True):
            st.code(code)
            st.info("Skopiuj powy≈ºszy kod")
    
    with col2:
        # Generuj plik do pobrania
        if st.button("Pobierz plik", key=f"download_{hash(code)}", use_container_width=True):
            filename = f"code_{language}.{language}"
            st.download_button(
                label="Pobierz", 
                data=code, 
                file_name=filename,
                mime="text/plain",
                key=f"dl_{hash(code)}"
            )

# === Serwis LLM ===
class LLMService:
    def __init__(self, api_key: str):
        self.api_key = api_key
        
        # Prosta pamiƒôƒá podrƒôczna dla powtarzajƒÖcych siƒô pyta≈Ñ
        self.cache = {}

    def count_tokens(self, text: str) -> int:
        """Oszacuj liczbƒô token√≥w dla Claude"""
        try:
            encoding = tiktoken.encoding_for_model("gpt-4")  # U≈ºywamy kodowania gpt-4 jako przybli≈ºenia
            return len(encoding.encode(text))
        except:
            # Fallback do cl100k_base, je≈õli okre≈õlone kodowanie nie jest dostƒôpne
            try:
                encoding = tiktoken.get_encoding("cl100k_base")
                return len(encoding.encode(text))
            except Exception as e:
                # Ostateczny fallback - prymitywne oszacowanie
                return len(text) // 4  # Bardzo zgrubne przybli≈ºenie: ~4 znaki na token

    def get_cache_key(self, messages, model, system_prompt, temperature):
        """Generuj klucz pamiƒôci podrƒôcznej dla zapytania LLM"""
        cache_input = f"{json.dumps(messages)}-{model}-{system_prompt}-{temperature}"
        return hashlib.md5(cache_input.encode()).hexdigest()

    def call_llm(self, 
                messages: List[Dict[str, Any]], 
                model: str = "anthropic/claude-3.7-sonnet:floor", 
                system_prompt: str = None, 
                temperature: float = 0.7, 
                max_tokens: int = 12000,
                use_cache: bool = True) -> Dict[str, Any]:
        """Wywo≈Çaj API LLM przez OpenRouter"""
        # Sprawd≈∫ pamiƒôƒá podrƒôcznƒÖ, je≈õli u≈ºywamy cachowania
        if use_cache and temperature < 0.1:  # Cachujemy tylko deterministyczne odpowiedzi
            cache_key = self.get_cache_key(messages, model, system_prompt, temperature)
            if cache_key in self.cache:
                return self.cache[cache_key]
        
        # Przygotuj wiadomo≈õci z promptem systemowym, je≈õli podano
        api_messages = []
        if system_prompt:
            api_messages.append({"role": "system", "content": system_prompt})
        
        # Dodaj wszystkie wiadomo≈õci
        for msg in messages:
            api_messages.append({"role": msg["role"], "content": msg["content"]})
        
        # Oblicz tokeny promptu
        prompt_text = system_prompt or ""
        for msg in messages:
            if isinstance(msg.get("content"), str):
                prompt_text += msg["content"]
        
        prompt_tokens = self.count_tokens(prompt_text)
        
        # Zdefiniuj parametry ponawiania
        max_retries = 3
        retry_delay = 2  # sekundy
        
        # Pr√≥ba wywo≈Çania API z ponawianiem
        for attempt in range(max_retries):
            try:
                api_payload = {
                    "model": model,
                    "messages": api_messages,
                    "temperature": temperature,
                    "max_tokens": max_tokens
                }
                
                response = requests.post(
                    "https://openrouter.ai/api/v1/chat/completions",
                    headers={
                        "Authorization": f"Bearer {self.api_key}",
                        "Content-Type": "application/json"
                    },
                    json=api_payload,
                    timeout=300  # Zwiƒôkszenie timeout do 5 minut
                )
                response.raise_for_status()
                result = response.json()
                
                # Oszacuj tokeny odpowiedzi
                response_content = result["choices"][0]["message"]["content"]
                completion_tokens = self.count_tokens(response_content)
                
                # Dodaj informacje o tokenach do wyniku
                result["usage"] = {
                    "prompt_tokens": prompt_tokens,
                    "completion_tokens": completion_tokens,
                    "total_tokens": prompt_tokens + completion_tokens
                }
                
                # Dodaj do pamiƒôci podrƒôcznej, je≈õli u≈ºywamy cachowania
                if use_cache and temperature < 0.1:
                    cache_key = self.get_cache_key(messages, model, system_prompt, temperature)
                    self.cache[cache_key] = result
                
                return result
            
            except requests.exceptions.RequestException as e:
                if attempt == max_retries - 1:  # Ostatnia pr√≥ba
                    raise Exception(f"Nie uda≈Ço siƒô po≈ÇƒÖczyƒá z API po {max_retries} pr√≥bach: {str(e)}")
                
                # Czekaj przed ponowieniem
                time.sleep(retry_delay * (2 ** attempt))  # Wyk≈Çadnicze wycofanie
    
    def call_llm_streaming(self, 
                         messages: List[Dict[str, Any]], 
                         model: str = "anthropic/claude-3.7-sonnet:floor", 
                         system_prompt: str = None, 
                         temperature: float = 0.7, 
                         max_tokens: int = 12000) -> Generator[str, None, None]:
        """Wywo≈Çaj API LLM przez OpenRouter ze streamingiem odpowiedzi"""
        
        # Przygotuj wiadomo≈õci z promptem systemowym, je≈õli podano
        api_messages = []
        if system_prompt:
            api_messages.append({"role": "system", "content": system_prompt})
        
        # Dodaj wszystkie wiadomo≈õci
        for msg in messages:
            api_messages.append({"role": msg["role"], "content": msg["content"]})
        
        # Oblicz tokeny promptu
        prompt_text = system_prompt or ""
        for msg in messages:
            if isinstance(msg.get("content"), str):
                prompt_text += msg["content"]
        
        prompt_tokens = self.count_tokens(prompt_text)
        
        api_payload = {
            "model": model,
            "messages": api_messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "stream": True
        }
        
        response = requests.post(
            "https://openrouter.ai/api/v1/chat/completions",
            headers={
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            },
            json=api_payload,
            stream=True,
            timeout=300  # Zwiƒôkszenie timeout do 5 minut
        )
        
        # Przygotuj zmienne do ≈õledzenia odpowiedzi
        full_response = ""
        completion_tokens = 0
        
        for line in response.iter_lines():
            if line:
                line = line.decode('utf-8')
                if line.startswith('data:'):
                    if line.strip() == 'data: [DONE]':
                        break
                    
                    try:
                        data = json.loads(line[5:])
                        if "choices" in data and len(data["choices"]) > 0:
                            delta = data["choices"][0].get("delta", {})
                            if "content" in delta:
                                content_chunk = delta["content"]
                                full_response += content_chunk
                                completion_tokens += self.count_tokens(content_chunk)
                                yield content_chunk
                    except json.JSONDecodeError:
                        pass
        
        # Dodajemy metadane na ko≈Ñcu, aby mog≈Çy zostaƒá wykorzystane przez wywo≈ÇujƒÖcy kod
        yield {
            "full_response": full_response,
            "usage": {
                "prompt_tokens": prompt_tokens,
                "completion_tokens": completion_tokens,
                "total_tokens": prompt_tokens + completion_tokens
            }
        }

# === Funkcje pomocnicze ===
def calculate_cost(model_id: str, prompt_tokens: int, completion_tokens: int) -> float:
    """Oblicz szacowany koszt zapytania w USD"""
    for model in MODEL_OPTIONS:
        if model["id"] == model_id:
            return (prompt_tokens / 1_000_000) * model["pricing"]["prompt"] + \
                   (completion_tokens / 1_000_000) * model["pricing"]["completion"]
    return 0.0  # W przypadku nieznalezienia modelu

def format_message_for_display(message: Dict[str, str]) -> str:
    """Formatuj wiadomo≈õƒá do wy≈õwietlenia w interfejsie, ze wsparciem dla blok√≥w kodu"""
    content = message.get("content", "")
    
    # Wyodrƒôbnianie i formatowanie blok√≥w kodu
    def replace_code_block(match):
        lang = match.group(1) or ""
        code = match.group(2)
        return f"```{lang}\n{code}\n```"
    
    # ZastƒÖp bloki kodu ze sk≈ÇadniƒÖ markdown
    content = re.sub(r"```(.*?)\n(.*?)```", replace_code_block, content, flags=re.DOTALL)
    
    return content

def get_conversation_title(messages: List[Dict[str, str]], llm_service: LLMService, api_key: str) -> str:
    """Wygeneruj tytu≈Ç dla nowej konwersacji na podstawie pierwszej wiadomo≈õci u≈ºytkownika"""
    if not messages:
        return f"Nowa konwersacja {datetime.now().strftime('%d-%m-%Y %H:%M')}"
    
    # U≈ºyj pierwszej wiadomo≈õci u≈ºytkownika jako podstawy tytu≈Çu
    user_message = next((m["content"] for m in messages if m["role"] == "user"), "")
    
    if len(user_message) > 40:
        # Skorzystaj z LLM, aby stworzyƒá kr√≥tki, opisowy tytu≈Ç
        try:
            response = llm_service.call_llm(
                messages=[
                    {"role": "user", "content": f"Utw√≥rz kr√≥tki, opisowy tytu≈Ç (max. 5 s≈Ç√≥w) dla nastƒôpujƒÖcej konwersacji, bez cudzys≈Çow√≥w: {user_message[:200]}..."}
                ],
                model="anthropic/claude-3.5-haiku:floor",  # Ta≈Ñszy model jest wystarczajƒÖcy do tworzenia tytu≈Ç√≥w
                system_prompt="Jeste≈õ pomocnym asystentem, kt√≥ry tworzy kr√≥tkie, opisowe tytu≈Çy konwersacji.",
                temperature=0.2,
                max_tokens=20,
                use_cache=True
            )
            title = response["choices"][0]["message"]["content"].strip().strip('"\'')
            # Usu≈Ñ znaki, kt√≥re mog≈Çyby sprawiaƒá problemy z interfejsem
            title = re.sub(r'[^\w\s\-.,]', '', title)
            return title[:40]
        except Exception:
            # W przypadku b≈Çƒôdu, wr√≥ƒá do domy≈õlnego tytu≈Çu
            pass
    
    # Domy≈õlnie u≈ºyj skr√≥conej wiadomo≈õci u≈ºytkownika
    return user_message[:40] + ("..." if len(user_message) > 40 else "")

def parse_code_blocks(content):
    """Wyodrƒôbnij bloki kodu z tre≈õci markdown"""
    code_blocks = []
    pattern = r"```([a-zA-Z0-9]*)\n(.*?)\n```"
    matches = re.findall(pattern, content, re.DOTALL)
    
    for lang, code in matches:
        # Standardowy jƒôzyk, je≈õli nie zosta≈Ç okre≈õlony
        language = lang.strip() if lang.strip() else "python"
        code_blocks.append({"language": language, "code": code})
    
    return code_blocks

# === Komponenty interfejsu u≈ºytkownika ===
def sidebar_component():
    """Komponent paska bocznego z konwersacjami i ustawieniami"""
    st.sidebar.title("AI Asystent Developera")
    
    # Ustawienia modelu
    with st.sidebar.expander("‚öôÔ∏è Ustawienia modelu", expanded=False):
        model_options = {model["id"]: f"{model['name']}" for model in MODEL_OPTIONS}
        selected_model = st.selectbox(
            "Model LLM",
            options=list(model_options.keys()),
            format_func=lambda x: model_options[x],
            index=0,
            key="model_selection"
        )
        
        # Poka≈º opis modelu
        for model in MODEL_OPTIONS:
            if model["id"] == selected_model:
                st.info(model["description"])
        
        temperature = st.slider(
            "Temperatura",
            min_value=0.0,
            max_value=1.0,
            value=st.session_state.get("temperature", 0.7),
            step=0.1,
            help="Wy≈ºsza warto≈õƒá = bardziej kreatywne odpowiedzi"
        )
        
        st.session_state["temperature"] = temperature
        
        custom_system_prompt = st.text_area(
            "Prompt systemowy (opcjonalnie)",
            value=st.session_state.get("custom_system_prompt", DEFAULT_SYSTEM_PROMPT),
            help="Dostosuj zachowanie asystenta"
        )
        
        if st.button("Zresetuj do domy≈õlnego"):
            custom_system_prompt = DEFAULT_SYSTEM_PROMPT
        
        st.session_state["custom_system_prompt"] = custom_system_prompt
    
    # Statystyki token√≥w
    if "token_usage" in st.session_state:
        with st.sidebar.expander("üìä Statystyki token√≥w", expanded=False):
            st.metric("Tokeny prompt", st.session_state["token_usage"]["prompt"])
            st.metric("Tokeny completion", st.session_state["token_usage"]["completion"])
            st.metric("Szacunkowy koszt", f"${st.session_state['token_usage']['cost']:.4f}")
    
    # Lista konwersacji
    db = st.session_state.get("db")
    if db:
        with st.sidebar.expander("üí¨ Konwersacje", expanded=True):
            conversations = db.get_conversations()
            
            if conversations:
                for conv in conversations:
                    col1, col2 = st.columns([4, 1])
                    with col1:
                        if st.button(conv["title"], key=f"conv_{conv['id']}", use_container_width=True):
                            st.session_state["current_conversation_id"] = conv["id"]
                            st.rerun()
                    with col2:
                        if st.button("üóëÔ∏è", key=f"del_{conv['id']}", help="Usu≈Ñ konwersacjƒô"):
                            db.delete_conversation(conv["id"])
                            if st.session_state.get("current_conversation_id") == conv["id"]:
                                st.session_state["current_conversation_id"] = None
                            st.rerun()
            else:
                st.write("Brak zapisanych konwersacji")
        
        # Przycisk nowej konwersacji
        if st.sidebar.button("‚ûï Nowa konwersacja", use_container_width=True):
            st.session_state["current_conversation_id"] = str(uuid.uuid4())
            # Wyczy≈õƒá za≈ÇƒÖczniki dla nowej konwersacji
            st.session_state["attachments"] = []
            st.rerun()

@requires_auth
def chat_component():
    """Komponent interfejsu czatu"""
    # Dodaj niestandardowy CSS dla przypiƒôtego paska wej≈õciowego
    st.markdown("""
    <style>
    /* Miejsce na pasek wej≈õciowy na dole */
    .main .block-container {
        padding-bottom: 80px;
    }
    
    /* Przytwierdzony pasek wej≈õciowy na dole ekranu */
    .stChatInputContainer {
        position: fixed;
        bottom: 0;
        left: 240px; /* Miejsce na sidebar */
        right: 0;
        padding: 1rem;
        background: white;
        z-index: 999;
        border-top: 1px solid #ddd;
    }
    
    /* Stylowanie za≈ÇƒÖcznik√≥w */
    .attachment-badge {
        display: inline-block;
        padding: 2px 8px;
        margin: 2px;
        background-color: #f0f2f6;
        border-radius: 10px;
        font-size: 0.8em;
    }
    
    /* Style dla blok√≥w kodu */
    .stMarkdown pre {
        overflow-x: auto;
    }
    
    /* Na urzƒÖdzeniach mobilnych */
    @media (max-width: 768px) {
        .stChatInputContainer {
            left: 0;
        }
    }
    </style>
    """, unsafe_allow_html=True)
    
    # Pobierz klucz API z secrets
    api_key = st.secrets.get("OPENROUTER_API_KEY", "")
    if not api_key:
        st.warning("‚ö†Ô∏è Brak klucza API OpenRouter w ustawieniach secrets.")
        return

    # Aktualizuj klucz API w sesji
    st.session_state["api_key"] = api_key

    # Pobierz instancjƒô serwisu LLM i bazy danych
    if "llm_service" not in st.session_state or st.session_state.get("llm_service") is None:
        st.session_state["llm_service"] = LLMService(api_key)

    llm_service = st.session_state.get("llm_service")
    db = st.session_state.get("db")

    if not llm_service or not db:
        st.error("‚ö†Ô∏è Nie mo≈ºna zainicjalizowaƒá serwis√≥w. Od≈õwie≈º stronƒô i spr√≥buj ponownie.")
        return

    # Inicjalizacja zmiennych sesji dla konwersacji
    if "current_conversation_id" not in st.session_state:
        st.session_state["current_conversation_id"] = str(uuid.uuid4())

    current_conversation_id = st.session_state["current_conversation_id"]

    # Statystyki konwersacji
    if "token_usage" not in st.session_state:
        st.session_state["token_usage"] = {"prompt": 0, "completion": 0, "cost": 0.0}
    
    # Inicjalizacja zmiennych dla za≈ÇƒÖcznik√≥w
    if "attachments" not in st.session_state:
        st.session_state["attachments"] = []

    # Pobierz wiadomo≈õci
    messages = db.get_messages(current_conversation_id)
    
    # Wy≈õwietl istniejƒÖce wiadomo≈õci
    for message in messages:
        role = message["role"]
        content = format_message_for_display(message)

        if role == "user":
            with st.chat_message("user"):
                st.markdown(content)
        elif role == "assistant":
            with st.chat_message("assistant"):
                # Sprawd≈∫, czy odpowied≈∫ zawiera bloki kodu do wy≈õwietlenia
                code_blocks = parse_code_blocks(content)
                
                # Wy≈õwietl tre≈õƒá og√≥lnƒÖ (bez blok√≥w kodu, kt√≥re bƒôdƒÖ wy≈õwietlane oddzielnie)
                clean_content = content
                for match in re.finditer(r"```.*?```", content, re.DOTALL):
                    start, end = match.span()
                    clean_content = clean_content.replace(match.group(), f"[BLOK KODU #{match.start()}]")
                
                st.markdown(clean_content)
                
                # Wy≈õwietl ka≈ºdy blok kodu z dodatkowymi kontrolkami
                if code_blocks:
                    st.write("### Bloki kodu:")
                    for i, block in enumerate(code_blocks):
                        with st.expander(f"Kod {i+1} - {block['language']}", expanded=True):
                            display_code_block(block['code'], block['language'])

    # Wy≈õwietlanie za≈ÇƒÖcznik√≥w (jako tekst pod polem wej≈õciowym)
    if st.session_state["attachments"]:
        attachment_text = "Za≈ÇƒÖczniki: " + " ".join([
            f"<span class='attachment-badge'>{attachment.get('type')} | {attachment.get('name')[:15]}...</span>"
            for attachment in st.session_state["attachments"]
        ])
        st.markdown(f"<div style='margin-bottom: 5px'>{attachment_text}</div>", unsafe_allow_html=True)
    
    # Obs≈Çuga za≈ÇƒÖcznik√≥w - ikony pod polem czatu
    col1, col2 = st.columns([1, 1])
    
    with col1:
        if st.button("üìÑ Plik", use_container_width=True):
            st.session_state["show_file_uploader"] = not st.session_state.get("show_file_uploader", False)
            st.rerun()
    
    with col2:
        if st.button("üíª Kod", use_container_width=True):
            st.session_state["show_code_input"] = not st.session_state.get("show_code_input", False)
            st.rerun()
    
    # ZarzƒÖdzanie istniejƒÖcymi za≈ÇƒÖcznikami
    if st.session_state["attachments"]:
        cols = st.columns(len(st.session_state["attachments"]))
        for i, (col, attachment) in enumerate(zip(cols, st.session_state["attachments"])):
            with col:
                if st.button(f"‚ùå {attachment.get('name', '')[:7]}...", key=f"del_{i}"):
                    st.session_state["attachments"].pop(i)
                    st.rerun()

    # Formularze za≈ÇƒÖcznik√≥w
    if st.session_state.get("show_file_uploader", False):
        with st.expander("Dodaj plik tekstowy", expanded=True):
            uploaded_file = st.file_uploader("Wybierz plik", type=["txt", "md", "json", "csv", "py", "js", "html", "css"], key="text_upload")
            col1, col2 = st.columns([1, 1])
            with col1:
                if st.button("Anuluj", use_container_width=True):
                    st.session_state["show_file_uploader"] = False
                    st.rerun()
            with col2:
                if uploaded_file is not None and st.button("Dodaj", use_container_width=True):
                    try:
                        text_content = uploaded_file.getvalue().decode("utf-8")
                        st.session_state["attachments"].append({
                            "type": "file",
                            "name": uploaded_file.name,
                            "text_content": text_content
                        })
                        st.session_state["show_file_uploader"] = False
                        st.success(f"Dodano plik: {uploaded_file.name}")
                        st.rerun()
                    except Exception as e:
                        st.error(f"B≈ÇƒÖd odczytu pliku: {str(e)}")

    if st.session_state.get("show_code_input", False):
        with st.expander("Dodaj kod", expanded=True):
            code_language = st.selectbox("Jƒôzyk programowania", 
                                         ["python", "javascript", "html", "css", "json", "sql", "bash"], 
                                         key="code_language")
            code_content = st.text_area("Wklej kod", height=150, key="code_content")
            file_name = st.text_input("Nazwa pliku (opcjonalnie)", 
                                      value=f"code.{code_language}", 
                                      key="code_filename")

            col1, col2 = st.columns([1, 1])
            with col1:
                if st.button("Anuluj", use_container_width=True):
                    st.session_state["show_code_input"] = False
                    st.rerun()
            with col2:
                if code_content and st.button("Dodaj", use_container_width=True):
                    st.session_state["attachments"].append({
                        "type": "file",
                        "name": file_name,
                        "text_content": f"```{code_language}\n{code_content}\n```"
                    })
                    st.session_state["show_code_input"] = False
                    st.success(f"Dodano kod: {file_name}")
                    st.rerun()
    
    # Pole wej≈õciowe u≈ºytkownika - ostatni element
    user_input = st.chat_input("Wpisz swoje pytanie lub zadanie...")

    # Obs≈Çuga wprowadzonego komunikatu
    if user_input:
        # Natychmiast wy≈õwietl wiadomo≈õƒá u≈ºytkownika
        with st.chat_message("user"):
            st.markdown(user_input)
        
        # Przygotuj tre≈õƒá wiadomo≈õci i za≈ÇƒÖczniki
        message_content = user_input
        
        # Przetw√≥rz za≈ÇƒÖczniki
        attachments_to_send = st.session_state.get("attachments", []).copy()
        
        # Dodaj informacje o za≈ÇƒÖcznikach do tre≈õci wiadomo≈õci
        attachments_text = process_attachments(attachments_to_send)
        if attachments_text:
            message_content += "\n\n" + attachments_text
        
        # Sprawd≈∫, czy konwersacja ma tytu≈Ç
        if len(messages) == 0:
            conversation_title = get_conversation_title([{"role": "user", "content": user_input}], llm_service, st.session_state["api_key"])
            db.save_conversation(current_conversation_id, conversation_title)
        
        # Zapisz wiadomo≈õƒá u≈ºytkownika w bazie danych
        db.save_message(current_conversation_id, "user", user_input, attachments_to_send)
        
        # Pobierz wszystkie wiadomo≈õci, aby zachowaƒá kontekst
        all_messages = db.get_messages(current_conversation_id)
        
        # Wy≈õwietl oczekujƒÖcƒÖ odpowied≈∫ asystenta
        with st.chat_message("assistant"):
            response_placeholder = st.empty()
            full_response = ""
            
            try:
                model = st.session_state.get("model_selection", MODEL_OPTIONS[0]["id"])
                system_prompt = st.session_state.get("custom_system_prompt", DEFAULT_SYSTEM_PROMPT)
                temperature = st.session_state.get("temperature", 0.7)
                
                # U≈ºyj funkcji optymalizujƒÖcej kontekst
                optimized_messages = prepare_messages_with_token_management(
                    all_messages, 
                    system_prompt, 
                    model, 
                    llm_service
                )
                
                # Wywo≈Çaj API ze streamingiem
                metadata = None
                for chunk in llm_service.call_llm_streaming(
                    messages=optimized_messages,
                    model=model,
                    system_prompt=None,  # System prompt jest ju≈º dodany w optimized_messages
                    temperature=temperature,
                    max_tokens=12000
                ):
                    # Sprawd≈∫, czy to ostatni element z metadanymi
                    if isinstance(chunk, dict) and "full_response" in chunk:
                        metadata = chunk
                        break
                    
                    # Aktualizuj odpowied≈∫ w czasie rzeczywistym
                    full_response += chunk
                    response_placeholder.markdown(full_response + "‚ñå")
                
                # Finalne wy≈õwietlenie odpowiedzi bez kursora
                response_placeholder.markdown(full_response)
                
                # Aktualizuj statystyki token√≥w, je≈õli mamy metadane
                if metadata and "usage" in metadata:
                    usage = metadata["usage"]
                    st.session_state["token_usage"]["prompt"] += usage["prompt_tokens"]
                    st.session_state["token_usage"]["completion"] += usage["completion_tokens"]
                    
                    # Oblicz koszt
                    cost = calculate_cost(
                        model, 
                        usage["prompt_tokens"], 
                        usage["completion_tokens"]
                    )
                    
                    st.session_state["token_usage"]["cost"] += cost
                
                # Wy≈õwietl bloki kodu oddzielnie, je≈õli istniejƒÖ
                code_blocks = parse_code_blocks(full_response)
                if code_blocks:
                    st.write("### Bloki kodu:")
                    for i, block in enumerate(code_blocks):
                        with st.expander(f"Kod {i+1} - {block['language']}", expanded=True):
                            display_code_block(block['code'], block['language'])
                
                # Zapisz odpowied≈∫ asystenta
                db.save_message(current_conversation_id, "assistant", full_response)
                
                # Wyczy≈õƒá za≈ÇƒÖczniki po wys≈Çaniu
                st.session_state["attachments"] = []
                # Ukryj formularze za≈ÇƒÖcznik√≥w
                st.session_state["show_file_uploader"] = False  
                st.session_state["show_code_input"] = False
                
                # Od≈õwie≈º stronƒô aby wy≈õwietliƒá nowƒÖ wiadomo≈õƒá bez spinner√≥w
                st.rerun()
                
            except Exception as e:
                st.error(f"WystƒÖpi≈Ç b≈ÇƒÖd: {str(e)}")
                st.code(traceback.format_exc())

# === G≈Ç√≥wna aplikacja ===
def main():
    st.set_page_config(
        page_title="AI Asystent Developera Streamlit",
        page_icon="ü§ñ",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    # Inicjalizacja serwis√≥w
    if "db" not in st.session_state:
        st.session_state["db"] = AssistantDB()
    
    # Sprawd≈∫ autentykacjƒô
    if not st.session_state.get("authenticated", False):
        login_page()
        return
    
    # Pobierz klucz API z secrets
    api_key = st.secrets.get("OPENROUTER_API_KEY", "")
    if api_key and "llm_service" not in st.session_state:
        st.session_state["llm_service"] = LLMService(api_key)
        st.session_state["api_key"] = api_key
    
    # Wy≈õwietl sidebar
    sidebar_component()
    
    # Wy≈õwietl g≈Ç√≥wny komponent czatu
    chat_component()

if __name__ == "__main__":
    main()
